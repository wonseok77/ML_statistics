# kdigital_ML_statistic
### 내재화 : 출처 - 혼자공부하는 머신러닝 & 딥러닝
### 머신러닝
- 1day
    - 01_도미와_빙어_구분하기
        - KNN 성능개선점 : k근접이웃 개수 설정 파라미터 : n_neighbors = 5 
        - 리스트 병합 : 그냥 + 쓰면된다 , 반복데이터 만들기 [1] * 35 +  [0] * 14
        - 이차원형태로 만들기 : 1. for문 이용, 2. 리스트컴프리헨션 + zip 함수, 3. np.column_stack((a,b))

    - 02_훈련데이터_테스트데이터_나누기
        - 샘플링편향 최소화 파라미터 : stratify (sklearn model_selection tran_test_split)
        - KNN 원인분석 : distances, indexes = kn.kneighbors([[25,150]]) 이웃과의 거리와 인덱스를 추출해보고 결과값의 원인에대해 분석해볼수있다
- 2day
    - 01_전처리
        - 넘파이 데이터 생성 : np.random.seed(123), index = np.arange(49), np.random.shuffle(index),np.ones(5), np.zeros(5), np.concatenate()
        - 특정 점 표시 : marker="^",color='red' 
    - 02_최근접이웃_회귀
        - 평균 절대값 오차 : mean_absolute_error
        - 과대적합 : 훈련데이터의 평가결과가 너무 높고, 테스트데이터의 평가가 너무 낮을 경우
        - 과소적합 : 훈련데이터의 평가결과가 낮고, 테스트데이터의 평가가 높은 경우 (사용된 전체 데이터의 갯수가 너무 작은 경우에 주로 발생)
        - 2차원 데이터 생성 : reshape(-1,1)
            - 1차원을 2차원으로 만들대는 첫번재 행은 전체 갯수, 두번재는 1이 됩니다
            - 전체 갯수를 모를 때는 -1 값을 사용하면 전체 행의 갯수를 체크합니다
        - 과적합 해소 방법(KNN)
            - 훈련모델을 복잡하게 만들어서 평가결과를 높일 수 있음
            - 복잡도를 높이는 방법은 이웃의 갯수를 줄여나가는 방법 입니다
            - 모델을 복잡하게 만드는 방법 : 이웃의 갯수를 작게하면 됩니다
            - 이웃의 갯수가 작아지면 , 이웃의 비교 대상이 조금 더 압축되기 때문에, 훈련의 집중도가 높아집니다 . 정확도가 높아집니다
            - 이웃의 갯수 : 기본5개
            - 이웃의 개수는 홀수로 써야함 최소 3이상의 홀수 why ? 짝수면 동률이 나옴 , 1이면 분석의 의미가 없다
    - 03_k-최근접이웃의_한계_및_선형회귀
        - 선형회귀 : 회귀모델 중 굉장히 막강해져가고 있는 모델, 라이브러리가 지속적으로 업데이트, 개선되고 있다
        - 선형회귀 분석 : lr.coef_ (기울기) , lr.intercept_ (y절편)
        - 다항회귀 : 2차항이 포함된 회귀식
        - 다항회귀 : 선형회귀의 결정력, 데이터 분포 등을 보고 성능향상을 위해 독립변수로 이차항을 추가하거나 해서 사용
        - 다항회귀 과적합 해결법 : 특성(독립변수 항목) 이 추가되어야 한다  
            
